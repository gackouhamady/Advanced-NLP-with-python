{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course Exercises notebook : Advanced NLP With Python for Machine Learning Training Courses (LinkedIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a spaCy Processing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise #1: Load Resources\n",
    "\n",
    "Load spaCy Resources  : \n",
    "\n",
    "\n",
    "- import pandas as pd\n",
    "- Import spaCy\n",
    "- Install spaCy\n",
    "- Download the English language model for spaCy\n",
    "- Load the English model\n",
    "  \n",
    "When you execute nlp = spacy.load('en'), spaCy downloads and loads the pre-trained English language model into memory and assigns it to the variable nlp. This pre-trained model contains information about word vectors, part-of-speech tags, syntactic dependencies, and other linguistic features necessary for various NLP tasks.\n",
    "\n",
    "#### spaCy Processing Pipeline\n",
    "\n",
    "In spaCy, the order of tasks in the processing pipeline generally follows a predefined sequence, although it's also customizable. By default, spaCy's processing pipeline includes the following components in the specified order:\n",
    "\n",
    "#### Order of Tasks in the Processing Pipeline\n",
    "\n",
    "| Order | Name                        | Definition                                                                                          |\n",
    "|-------|-----------------------------|-----------------------------------------------------------------------------------------------------|\n",
    "| 1     | Tokenization                | Input text is split into individual tokens, such as words and punctuation marks.                   |\n",
    "| 2     | Stop Words                  | Removes stop words from the text.                                                                   |\n",
    "| 3     | POS Tagging                 | Assigns grammatical labels (e.g., noun, verb, adjective) to each token in the text based on its syntactic role within the sentence. |\n",
    "| 4     | Dependency Parsing          | Analyzes the grammatical structure of the text by determining the relationships between tokens.     |\n",
    "| 5     | Lemmatization               | Reduces tokens to their base or root form (lemmas).                                                 |\n",
    "| 6     | Named Entity Recognition    | Identifies and categorizes persons, organizations, locations, dates, etc.                           |\n",
    "| 7     | Other Use Case Tasks        | May be included in the pipeline (e.g., Sentiment Analysis).                                        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: green;\">Solution1</h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:\n",
      "SpaCy\n",
      "is\n",
      "an\n",
      "open\n",
      "-\n",
      "source\n",
      "library\n",
      "for\n",
      "advanced\n",
      "Natural\n",
      "Language\n",
      "Processing\n",
      ".\n",
      "\n",
      "Named Entities:\n",
      "SpaCy PERSON\n",
      "Natural Language Processing WORK_OF_ART\n",
      "\n",
      "Part-of-Speech Tags:\n",
      "SpaCy: PROPN\n",
      "is: AUX\n",
      "an: DET\n",
      "open: ADJ\n",
      "-: PUNCT\n",
      "source: NOUN\n",
      "library: NOUN\n",
      "for: ADP\n",
      "advanced: ADJ\n",
      "Natural: PROPN\n",
      "Language: PROPN\n",
      "Processing: NOUN\n",
      ".: PUNCT\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Install spaCy (run this once in terminal)\n",
    "# pip install spacy\n",
    "# python -m spacy download en_core_web_md\n",
    "\n",
    "# Step 2: Import libraries\n",
    "import pandas as pd  # For data handling (optional for this exercise)\n",
    "import spacy  # For NLP tasks\n",
    "\n",
    "# Step 3: Load the English model\n",
    "nlp = spacy.load('en_core_web_sm')  # Load the English language model\n",
    "\n",
    "# Step 4: Example text\n",
    "text = \"SpaCy is an open-source library for advanced Natural Language Processing.\"\n",
    "\n",
    "# Step 5: Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Step 6: Output the results\n",
    "print(\"Tokens:\")\n",
    "for token in doc:\n",
    "    print(token.text)\n",
    "\n",
    "print(\"\\nNamed Entities:\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "\n",
    "print(\"\\nPart-of-Speech Tags:\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text}: {token.pos_}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": ".env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
